{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eacc5988-7526-4575-8841-13e54c6c702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans # KMedoids clust = KMedoids(n_clusters=20, random_state=0, metric='euclidean').fit(d1)\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os, yaml, sys\n",
    "ENV = os.getenv(\"MY_ENV\", \"dev\")\n",
    "with open(\"../../config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "paths = config[ENV][\"paths\"]\n",
    "sys.path.append(paths[\"src_path\"])\n",
    "from dim_redu_anns.utils import get_relevant_output_layers, get_layer_out_shape\n",
    "from sampling.sampling_comparisons import sum_for_var, variance\n",
    "from sklearn.cluster import KMeans\n",
    "from torchvision.models.feature_extraction import (\n",
    "    create_feature_extractor,\n",
    "    get_graph_node_names,\n",
    ")\n",
    "import torch\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from alignment.utils import get_usual_transform\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c54ec3-cbd4-43ab-a15e-12f26adb52a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2load = f\"{paths['results_path']}/imagenet_val_alexnet_features.0_maxpool_features.pkl\"\n",
    "mp_data = joblib.load(path2load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ffbd35a-5c47-41ff-bfdb-93785a5653a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = get_usual_transform()\n",
    "batch_size = 10; num_workers = 1\n",
    "loader = DataLoader(\n",
    "    datasets.ImageFolder(f\"{paths['data_path']}/imagenet/val\", transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    timeout=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3adb00b2-52d1-4fc3-8aa1-af01e8d54fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tizianocausin/Desktop/virtual_envs/ponce_env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/tizianocausin/Desktop/virtual_envs/ponce_env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"alexnet\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_cls = getattr(models, model_name)\n",
    "model = model_cls(pretrained=True).to(device).eval()\n",
    "target_layer = \"features.0\"\n",
    "feature_extractor = create_feature_extractor(\n",
    "    model, return_nodes=[target_layer]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d4cb268-3ec3-425b-be69-b59dfb82c1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tizianocausin/Desktop/virtual_envs/ponce_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:32:52 starting batch 1\n",
      "12:32:52 starting batch 2\n",
      "12:32:52 starting batch 3\n",
      "12:32:52 starting batch 4\n",
      "12:32:52 starting batch 5\n",
      "12:32:52 starting batch 6\n",
      "12:32:52 starting batch 7\n",
      "12:32:52 starting batch 8\n",
      "12:32:52 starting batch 9\n",
      "12:32:52 starting batch 10\n",
      "4.242171148774216\n"
     ]
    }
   ],
   "source": [
    "num_stim = 100\n",
    "counter = 0\n",
    "d = get_layer_out_shape(feature_extractor, target_layer)\n",
    "tot_s = np.zeros(np.prod(d))\n",
    "tot_ss = np.zeros(np.prod(d))\n",
    "for inputs, _ in loader:\n",
    "    counter += 1\n",
    "    if counter*batch_size > num_stim:\n",
    "        break\n",
    "    # if counter*batch_size > num_stim:\n",
    "    print(datetime.now().strftime(\"%H:%M:%S\"), f\"starting batch {counter}\", flush=True)\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(device)\n",
    "        feats = feature_extractor(inputs)[target_layer]\n",
    "        feats = feats.view(feats.size(0), -1).cpu().numpy()\n",
    "        s, ss = sum_for_var(feats)\n",
    "        tot_s += s\n",
    "        tot_ss += ss\n",
    "    # with torch.no_grad():\n",
    "# for inputs, _ in loader:\n",
    "var = np.mean(variance(tot_s, tot_ss, num_stim))\n",
    "print(np.mean(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f89c865a-675c-4c0e-a1e4-c6afab8a6a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:32:59 starting batch 1\n",
      "12:32:59 starting batch 2\n",
      "12:32:59 starting batch 3\n",
      "12:32:59 starting batch 4\n",
      "12:32:59 starting batch 5\n",
      "12:32:59 starting batch 6\n",
      "12:32:59 starting batch 7\n",
      "12:32:59 starting batch 8\n",
      "12:32:59 starting batch 9\n",
      "12:32:59 starting batch 10\n"
     ]
    }
   ],
   "source": [
    "num_stim = 100\n",
    "counter = 0\n",
    "d = get_layer_out_shape(feature_extractor, target_layer)\n",
    "tot_s = np.zeros(np.prod(d))\n",
    "tot_ss = np.zeros(np.prod(d))\n",
    "tot_feats = []\n",
    "for inputs, _ in loader:\n",
    "    counter += 1\n",
    "    if counter*batch_size > num_stim:\n",
    "        break\n",
    "    # if counter*batch_size > num_stim:\n",
    "    print(datetime.now().strftime(\"%H:%M:%S\"), f\"starting batch {counter}\", flush=True)\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(device)\n",
    "        feats = feature_extractor(inputs)[target_layer]\n",
    "        feats = feats.view(feats.size(0), -1).cpu().numpy()\n",
    "        tot_feats.append(feats)#s, ss = sum_for_var(feats)\n",
    "        #tot_s += s\n",
    "        #tot_ss += ss\n",
    "    # with torch.no_grad():\n",
    "# for inputs, _ in loader:\n",
    "a = np.var(np.vstack(tot_feats), axis=0, ddof=1).mean()\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ponce_env",
   "language": "python",
   "name": "ponce_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
