{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9973efa-bdd1-40a3-af73-d11456afe120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from torchvision.datasets import ImageNet\n",
    "from torchvision import transforms\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/Users/tizianocausin/Desktop/backUp20240609/summer2025/ponce_lab/exp_set/python_scripts/src\")\n",
    "from sparsity_in_silico.sparsity_CNN import response_prob\n",
    "#import imageio\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef196a8-071d-45c9-af00-40f43c0ffef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tizianocausin/Desktop/backUp20240609/summer2025/ponce_lab/exp_set/python_scripts/scripts'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e20acf9b-ced1-4040-881d-97e8f10229a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tizianocausin/Desktop/virtual_envs/ponce_env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#alexnet = models.alexnet(pretrained=True).eval()\n",
    "alexnet = models.alexnet(weights=True).eval()\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.3806, 0.4242, 0.3794], std=[0.2447, 0.2732, 0.2561]\n",
    "        ),  # Normalization for pretrained model\n",
    "    ]\n",
    ")\n",
    "\n",
    "#reader = imageio.get_reader(path2vid)\n",
    "conv_layers = [\n",
    "    \"conv_layer1\",\n",
    "    \"conv_layer4\",\n",
    "    \"conv_layer7\",\n",
    "    \"conv_layer9\",\n",
    "    \"conv_layer11\",\n",
    "]\n",
    "conv_layers_idx = [1, 4, 7, 9, 11]\n",
    "\n",
    "fc_layers = [\"fc_layer2\", \"fc_layer5\"]\n",
    "fc_layers_idx = [2, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d80a089f-f117-4166-848e-bfc0c8020c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = create_feature_extractor(alexnet, return_nodes={\"features.0\" : \"1st\", \"features.1\" : \"2nd\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc370304-f211-4746-9774-d680d5ce3a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(get_graph_node_names(alexnet))\n",
    "layers = ['features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'avgpool', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6'] # took off the input and the \"flatten\" layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5201a91d-ffc1-441f-8308-7507166483eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0\n",
      "features.1\n",
      "features.2\n",
      "features.3\n",
      "features.4\n",
      "features.5\n",
      "features.6\n",
      "features.7\n",
      "features.8\n",
      "features.9\n",
      "features.10\n",
      "features.11\n",
      "features.12\n",
      "avgpool\n",
      "classifier.0\n",
      "classifier.1\n",
      "classifier.2\n",
      "classifier.3\n",
      "classifier.4\n",
      "classifier.5\n",
      "classifier.6\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(layers), 1):\n",
    "    curr_layer = layers[i]\n",
    "    print(curr_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b2a0d652-4e1c-47d7-a065-0511dc8d28be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d60db2e-464b-42a2-970f-e19b0f08ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_set = ImageNet(root=\"/Users/tizianocausin/Library/CloudStorage/OneDrive-SISSA/data_repo/exp_set_data/imagenet\", split=\"val\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92029659-933d-4900-ad8b-37751ff27877",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=1,         \n",
    "    shuffle=False,         \n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29ce400e-bb80-4e3e-9726-4397c22854ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = {\"1st\" : [], \"2nd\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c0a4028-11d0-4a5a-bba9-85559b4b2851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st\n",
      "2nd\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():  # Disable gradient tracking for evaluation\n",
    "    for inputs, labels in val_loader:\n",
    "\n",
    "        f = feature_extractor(inputs)\n",
    "        #outputs = alexnet(inputs)           # Forward pass\n",
    "        #_, preds = torch.max(outputs, 1)  # Get predicted class indices\n",
    "        for layer, repr in f.items():\n",
    "            print(layer)\n",
    "            # flat_repr = repr.reshape(-1).detach().cpu().numpy().astype(np.float16)\n",
    "            #feats[layer].append(flat_repr)\n",
    "            feats[layer].append(repr)\n",
    "\n",
    "        # Example: print first batch predictions\n",
    "        \n",
    "        break  # Remove this to go through the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b523e80-4551-402c-8656-513e4676468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_arr = feats[\"1st\"][0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0c3e7a5a-400e-4fe9-9652-3f310d894f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 64, 55, 55)\n",
      "(1, 55)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(my_arr))\n",
    "print(np.max(my_arr, axis=(1,2)).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fd6c04a-28dd-41d2-a6f4-01c78c7d0a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_layer1 torch.Size([193600])\n",
      "conv_layer4 torch.Size([139968])\n",
      "conv_layer7 torch.Size([64896])\n",
      "conv_layer9 torch.Size([43264])\n",
      "conv_layer11 torch.Size([43264])\n",
      "fc_layer2 torch.Size([4096])\n",
      "fc_layer5 torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "for k, v in all_feats.items():\n",
    "    print(k, v[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8f04d59-be9f-408e-8cc5-1486a2b4a5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_layer1 torch.Size([3872])\n",
      "conv_layer4 torch.Size([2799])\n",
      "conv_layer7 torch.Size([1297])\n",
      "conv_layer9 torch.Size([865])\n",
      "conv_layer11 torch.Size([865])\n",
      "fc_layer2 torch.Size([81])\n",
      "fc_layer5 torch.Size([81])\n"
     ]
    }
   ],
   "source": [
    "a = response_prob(feats)\n",
    "for k, v in a.items():\n",
    "    print(k, v.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e9ba36d2-b439-4144-9e19-c41469c34b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2res = \"/Users/tizianocausin/Library/CloudStorage/OneDrive-SISSA/data_repo/exp_set_res/silico\"\n",
    "with h5py.File(f\"{path2res}/dummy_feats_alexnet.h5\", \"a\") as f:\n",
    "# Iterate over dictionary items and save them in the HDF5 file\n",
    "    for key, value in feats.items():\n",
    "        f.create_dataset(\n",
    "            key, data=value\n",
    "        )  # Create a dataset for each key-value pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d03d5c8-e4d3-4453-83ba-c6e850803dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ponce_env",
   "language": "python",
   "name": "ponce_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
